{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, progress, get_worker\n",
    "import os\n",
    "import binascii\n",
    "\n",
    "# Replace with IP address of the dask scheduler\n",
    "client = Client(\"tcp://10.10.24.60:8786\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants\n",
    "BATCH_SIZES = [100, 500, 1000, 1500, 2000]\n",
    "\n",
    "XCLBIN_PATH = \"a.xclbin\"\n",
    "PLATFORM = \"alveo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-11-12 13:59:59--  https://raw.githubusercontent.com/modestyachts/CIFAR-10.1/master/datasets/cifar10.1_v4_data.npy\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.36.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.36.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6208640 (5.9M) [application/octet-stream]\n",
      "Saving to: ‘cifar10.1_v4_data.npy.3’\n",
      "\n",
      "100%[======================================>] 6,208,640   34.0MB/s   in 0.2s   \n",
      "\n",
      "2020-11-12 14:00:00 (34.0 MB/s) - ‘cifar10.1_v4_data.npy.3’ saved [6208640/6208640]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Download the CIFAR 10 dataset \n",
    "!wget https://raw.githubusercontent.com/modestyachts/CIFAR-10.1/master/datasets/cifar10.1_v4_data.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_on_worker(ibuf_normal, index):\n",
    "    print(\"Received \", len(ibuf_normal), \"images for classification\")\n",
    "    from multiprocessing import Process,Queue\n",
    "    import numpy as np\n",
    "    import time\n",
    "    \n",
    "    def forked_process(queue, ibuf_normal):\n",
    "        from driver import FINNAccelDriver\n",
    "        from pynq.ps import Clocks\n",
    "        \n",
    "        batch_size = len(ibuf_normal)\n",
    "        t0 = time.time()\n",
    "        finnDriver = FINNAccelDriver(batch_size, XCLBIN_PATH, PLATFORM)\n",
    "        ibuf_folded = finnDriver.fold_input(ibuf_normal)\n",
    "#         ibuf_packed = finnDriver.pack_input(ibuf_folded)   Do not pack for performance reasons\n",
    "        ibuf_packed = ibuf_folded\n",
    "        finnDriver.copy_input_data_to_device(ibuf_packed)\n",
    "        finnDriver.execute()\n",
    "        obuf_packed = np.empty_like(finnDriver.obuf_packed_device)\n",
    "        finnDriver.copy_output_data_from_device(obuf_packed)\n",
    "        obuf_folded = finnDriver.unpack_output(obuf_packed)\n",
    "        obuf_normal = finnDriver.unfold_output(obuf_folded)\n",
    "        t1 = time.time()\n",
    "        \n",
    "        if PLATFORM != \"alveo\":\n",
    "            fclk_mhz = Clocks.fclk0_mhz\n",
    "        else:\n",
    "            fclk_mhz = finnDriver.fclk_mhz\n",
    "        runtime = t1-t0\n",
    "        queue.put({\n",
    "            'data': obuf_normal,\n",
    "            'runtime': runtime,\n",
    "            'index': index,\n",
    "            'fclk_mhz': fclk_mhz,\n",
    "            'throughput': batch_size/runtime,\n",
    "            'bandwidth_in': np.prod(finnDriver.ishape_packed)*0.000001 / runtime,\n",
    "            'bandwidth_out': np.prod(finnDriver.oshape_packed)*0.000001 / runtime,\n",
    "            'N': batch_size\n",
    "            \n",
    "        })\n",
    "    \n",
    "    \n",
    "    # We need to run the Pynq overlay in a new forked process since it cannot be run in a non-Main thread\n",
    "    t0_total = time.time()\n",
    "    queue = Queue()\n",
    "    p = Process(target=forked_process, args=(queue, ibuf_normal))\n",
    "    p.start()\n",
    "    result = queue.get()\n",
    "    p.join()\n",
    "    t1_total = time.time()\n",
    "    print(\"TOTAL EXECUTION TIME ON THIS WORKER (s): \", t1_total - t0_total)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_SIZE: 100\n",
      "Sending data to workers, and triggering worker tasks...\n",
      "Received data from workers.\n",
      "TOTAL EXECUTION TIME: 1.3151390552520752\n",
      "Maximum FPGA runtime[s]: 0.2621912956237793\n",
      "Average throughput[images/s]: 381.40091478662555\n",
      "Average DRAM_in_bandwidth[Mb/s]: 1.1716636102245137\n",
      "Average DRAM_out_bandwidth[Mb/s]: 0.0003814009147866255\n",
      "**************************\n",
      "BATCH_SIZE: 500\n",
      "Sending data to workers, and triggering worker tasks...\n",
      "Received data from workers.\n",
      "TOTAL EXECUTION TIME: 1.4434535503387451\n",
      "Maximum FPGA runtime[s]: 0.4252645969390869\n",
      "Average throughput[images/s]: 1175.7385956856829\n",
      "Average DRAM_in_bandwidth[Mb/s]: 3.611868965946418\n",
      "Average DRAM_out_bandwidth[Mb/s]: 0.0011757385956856828\n",
      "**************************\n",
      "BATCH_SIZE: 1000\n",
      "Sending data to workers, and triggering worker tasks...\n",
      "Received data from workers.\n",
      "TOTAL EXECUTION TIME: 1.6121728420257568\n",
      "Maximum FPGA runtime[s]: 0.6024956703186035\n",
      "Average throughput[images/s]: 1659.7629647217111\n",
      "Average DRAM_in_bandwidth[Mb/s]: 5.098791827625097\n",
      "Average DRAM_out_bandwidth[Mb/s]: 0.0016597629647217111\n",
      "**************************\n",
      "BATCH_SIZE: 1500\n",
      "Sending data to workers, and triggering worker tasks...\n",
      "Received data from workers.\n",
      "TOTAL EXECUTION TIME: 1.641434907913208\n",
      "Maximum FPGA runtime[s]: 0.7566468715667725\n",
      "Average throughput[images/s]: 1982.4307168467928\n",
      "Average DRAM_in_bandwidth[Mb/s]: 6.090027162153348\n",
      "Average DRAM_out_bandwidth[Mb/s]: 0.001982430716846793\n",
      "**************************\n",
      "BATCH_SIZE: 2000\n",
      "Sending data to workers, and triggering worker tasks...\n",
      "Received data from workers.\n",
      "TOTAL EXECUTION TIME: 1.9337224960327148\n",
      "Maximum FPGA runtime[s]: 0.9211459159851074\n",
      "Average throughput[images/s]: 2171.2086709532073\n",
      "Average DRAM_in_bandwidth[Mb/s]: 6.669953037168254\n",
      "Average DRAM_out_bandwidth[Mb/s]: 0.0021712086709532075\n",
      "**************************\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "num_of_workers = len(client.scheduler_info()[\"workers\"])\n",
    "full_cifar = np.load('cifar10.1_v4_data.npy')\n",
    "\n",
    "\n",
    "for BATCH_SIZE in BATCH_SIZES:\n",
    "    print(\"BATCH_SIZE:\", BATCH_SIZE)\n",
    "    partial_cifar = full_cifar[:BATCH_SIZE]\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # Split up the file into equal sized chunks based on number of available dask workers\n",
    "    data_split = []\n",
    "    start = 0\n",
    "    chunk_size = int(len(partial_cifar)/num_of_workers)\n",
    "    for i in range(num_of_workers - 1):\n",
    "        data_split.append(partial_cifar[start: start+chunk_size])\n",
    "        start += chunk_size\n",
    "    data_split.append(partial_cifar[start:]) #Last partition\n",
    "\n",
    "    # Scatter the data to the workers before calling run_on_worker on the workers\n",
    "    print(\"Sending data to workers, and triggering worker tasks...\")\n",
    "    \n",
    "    distributed_data = client.scatter(data_split)\n",
    "    futures = client.map(run_on_worker, distributed_data, range(num_of_workers))\n",
    "    results = client.gather(futures)\n",
    "    print(\"Received data from workers.\")\n",
    "\n",
    "    # Reorder the response based on original input order\n",
    "    results.sort(key = lambda result: result['index'])  \n",
    "\n",
    "    # Concatenate the result where each is an ndarray of the shape (BATCH_SIZE/num_of_workers, 1)\n",
    "    merged_result = np.concatenate([r['data'] for r in results]) # FINAL RESULTS (CLASS LABELS)\n",
    "    \n",
    "    t1 = time.time()\n",
    "\n",
    "    def avg(li):\n",
    "        return sum(li)/len(li)\n",
    "\n",
    "    print(\"TOTAL EXECUTION TIME:\", t1-t0)\n",
    "    print(\"Maximum FPGA runtime[s]:\", max([r['runtime'] for r in results])) # Shown in the plot\n",
    "    print(\"Average throughput[images/s]:\", avg([r['throughput'] for r in results]))\n",
    "    print(\"Average DRAM_in_bandwidth[Mb/s]:\", avg([r['bandwidth_in'] for r in results])) \n",
    "    print(\"Average DRAM_out_bandwidth[Mb/s]:\", avg([r['bandwidth_out'] for r in results]))    \n",
    "    print(\"**************************\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
