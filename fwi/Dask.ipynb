{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Introduction </h1>\n",
    "\n",
    "This notebook demonstrates how to scale an implemenation of Full Waveform Inversion using multiple FPGAs with bitstreams containing one or multiple copied compute units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Define experiment parameters: </h3>\n",
    "    \n",
    "XCLBIN_PATH_DEFAULT => Default path for the .xclbin file containing 1 compute unit  \n",
    "XCLBIN_PATH_MULTCU => Default path for .xclbin file containing 2 copied compute units  \n",
    "XRT_ENV_PATH => Path to the Xilinx Runtime setup script.  \n",
    "DEVICE_NAME_DEFAULT => Default name for the FPGA device  \n",
    "DIR_PATH => Path to the directory containing the FWI input files.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "XCLBIN_PATH_DEFAULT = \"/bitstreams/u280_xclbin/500_500_HBM/FullW.xclbin\"\n",
    "XCLBIN_PATH_MULTCU = \"/bitstreams/u280_xclbin/500_250_HBM/FullW.xclbin\"\n",
    "XRT_ENV_PATH = \"/opt/xilinx/xrt/setup.sh\"\n",
    "DEVICE_NAME_DEFAULT=\"xilinx_u280_xdma_201920_3\"\n",
    "\n",
    "DIR_PATH = \"default/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Define the worker method </h3> \n",
    "\n",
    "Here, we define the Python method which will be executed on each of the Dask workers. This function calls the driver using the data partition it receives, and returns the output data (along with some performance statistics) to the caller (the Dask client). \n",
    "\n",
    "We present two methods, the first can be used to execute single compute unit bitstreams, the second demonstrates how we can use multiple dask workers with the same bitstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_function(queue, grid_data,kernel,path,index,config):\n",
    "    import numpy as np\n",
    "    import time\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    from pynq import Overlay, allocate, Device, lib\n",
    "    from FWIDriver import FWI\n",
    "\n",
    "    resolution = config[\"Freq\"][\"nTotal\"] * config[\"nSources\"] * config[\"nReceivers\"]\n",
    "    gridsize = len(grid_data)   \n",
    "    config[\"tolerance\"] = 9.99*10**-7\n",
    "    config[\"max\"] = 1000\n",
    "\n",
    "    # Load the overlay\n",
    "    devices = Device.devices\n",
    "    ol = Overlay(path, download=True, device=devices[0])\n",
    "\n",
    "    # Allocate the buffers\n",
    "    A = allocate(shape=(resolution,gridsize), dtype=np.complex64, target=getattr(ol,kernel[0][\"dotprod_\"+index][0]))\n",
    "    B = allocate(shape=(gridsize,), dtype=np.float32, target=getattr(ol,kernel[0][\"dotprod_\"+index][1]))\n",
    "    C = allocate(shape=(resolution,), dtype=np.complex64, target=getattr(ol,kernel[0][\"dotprod_\"+index][2]))\n",
    "\n",
    "    D = allocate(shape=(resolution,gridsize), dtype=np.complex64,  target=getattr(ol,kernel[1][\"update_\"+index][0]))\n",
    "    E = allocate(shape=(resolution,),dtype=np.complex64,  target=getattr(ol,kernel[1][\"update_\"+index][1]))\n",
    "    F = allocate(shape=(gridsize), dtype=np.complex64, target=getattr(ol,kernel[1][\"update_\"+index][2]))\n",
    "\n",
    "    # set up the kernel IP's\n",
    "    dotprod = getattr(ol,\"dotprod_\"+index)\n",
    "    update = getattr(ol,\"update_\"+index)\n",
    "\n",
    "\n",
    "    fwi = FWI(A,B,C,D,E,F,dotprod,update,config,resolution,gridsize,True)\n",
    "\n",
    "    # pre process the grid data\n",
    "    fwi.pre_process(grid_data)\n",
    "\n",
    "    # reconstruct the grid by performing Full Wavefrom Inversion\n",
    "    chi = fwi.reconstruct()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    # free all the buffers\n",
    "    A.freebuffer()\n",
    "    B.freebuffer()\n",
    "    C.freebuffer()\n",
    "    D.freebuffer()\n",
    "    E.freebuffer()\n",
    "    F.freebuffer()\n",
    "    ol.free()\n",
    "    \n",
    "    dict_t = {\n",
    "    \"index\": index,\n",
    "    \"time\": total_time,\n",
    "    \"chi\":chi\n",
    "    }\n",
    "    return dict_t\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_on_worker(grid_data,kernel,index):\n",
    "    from dask import delayed, compute\n",
    "    from pynq import Overlay, allocate, Device, lib\n",
    "    from multiprocessing import Queue, Process\n",
    "\n",
    "    devices = Device.devices\n",
    "    ol = Overlay(kernel[\"path_to_kernel\"], download=True, device=devices[0])\n",
    "    \n",
    "    def execute_function(queue, grid_data,kernel,path,index,config):\n",
    "        import numpy as np\n",
    "        import time\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "\n",
    "        from pynq import Overlay, allocate, Device, lib\n",
    "        from FWIDriver import FWI\n",
    "        \n",
    "        \n",
    "        resolution = config[\"Freq\"][\"nTotal\"] * config[\"nSources\"] * config[\"nReceivers\"]\n",
    "        gridsize = len(grid_data)   \n",
    "        config[\"tolerance\"] = 9.99*10**-7\n",
    "        config[\"max\"] = 1000\n",
    "\n",
    "        # Load the overlay\n",
    "        devices = Device.devices\n",
    "        ol = Overlay(path, download=True, device=devices[0])\n",
    "        \n",
    "        # Allocate the buffers\n",
    "        A = allocate(shape=(resolution,gridsize), dtype=np.complex64, target=getattr(ol,kernel[0][\"dotprod_\"+index][0]))\n",
    "        B = allocate(shape=(gridsize,), dtype=np.float32, target=getattr(ol,kernel[0][\"dotprod_\"+index][1]))\n",
    "        C = allocate(shape=(resolution,), dtype=np.complex64, target=getattr(ol,kernel[0][\"dotprod_\"+index][2]))\n",
    "\n",
    "        D = allocate(shape=(resolution,gridsize), dtype=np.complex64,  target=getattr(ol,kernel[1][\"update_\"+index][0]))\n",
    "        E = allocate(shape=(resolution,),dtype=np.complex64,  target=getattr(ol,kernel[1][\"update_\"+index][1]))\n",
    "        F = allocate(shape=(gridsize), dtype=np.complex64, target=getattr(ol,kernel[1][\"update_\"+index][2]))\n",
    "\n",
    "        # set up the kernel IP's\n",
    "        dotprod = getattr(ol,\"dotprod_\"+index)\n",
    "        update = getattr(ol,\"update_\"+index)\n",
    "        \n",
    "        \n",
    "        fwi = FWI(A,B,C,D,E,F,dotprod,update,config,resolution,gridsize,True)\n",
    "        \n",
    "        # pre process the grid data\n",
    "        fwi.pre_process(grid_data)\n",
    "        \n",
    "        # reconstruct the grid by performing Full Wavefrom Inversion\n",
    "        chi = fwi.reconstruct()\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "\n",
    "        # free all the buffers\n",
    "        A.freebuffer()\n",
    "        B.freebuffer()\n",
    "        C.freebuffer()\n",
    "        D.freebuffer()\n",
    "        E.freebuffer()\n",
    "        F.freebuffer()\n",
    "        \n",
    "        dict_t = {\n",
    "        \"index\": index,\n",
    "        \"time\": total_time,\n",
    "        \"chi\":chi\n",
    "        }\n",
    "        queue.put(dict_t)\n",
    "    \n",
    "\n",
    "    # Create a subprocess to handle multiple compute units, if we only use single compute units bitstreams we can map the execute functions directly\n",
    "    # TODO: example of that?\n",
    "    if kernel[\"no_instances\"]==1:\n",
    "        q = Queue()\n",
    "        cu = 0\n",
    "        p = Process(target=execute_function,args=(q, grid_data[cu],kernel[\"functions\"][cu],kernel[\"path_to_kernel\"],str(cu+1),kernel[\"config\"]))\n",
    "        p.start()\n",
    "        result = q.get()\n",
    "        p.join()\n",
    "    else:\n",
    "        p = []\n",
    "        q = []\n",
    "        result = []\n",
    "        for cu in range(kernel[\"no_instances\"]):\n",
    "            q.append(Queue())\n",
    "            p.append(Process(target=execute_function,args=(q[cu], grid_data[cu],kernel[\"functions\"][cu],kernel[\"path_to_kernel\"],str(cu+1),kernel[\"config\"])))\n",
    "            p[cu].start()\n",
    "        for cu in range(kernel[\"no_instances\"]):\n",
    "            result.append(q[cu].get())\n",
    "        for cu in range(kernel[\"no_instances\"]):\n",
    "            p[cu].join()\n",
    "     \n",
    "    # Free the overlay and return the results\n",
    "    ol.free()\n",
    "    return result\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Octoray import Octoray\n",
    "\n",
    "# Create an octoray instance with the \n",
    "octoray = Octoray(ssh_cluster=True, scheduler=\"10.1.212.126\",scheduler_port=8786, hosts=[\"10.1.212.127\"], config_file=\"cluster_config.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing OctoRay with client ip: 10.1.212.126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.deploy.ssh - INFO - distributed.scheduler - INFO - Clear task state\n",
      "distributed.deploy.ssh - INFO - distributed.scheduler - INFO -   Scheduler at:   tcp://10.1.212.126:8786\n",
      "distributed.deploy.ssh - INFO - distributed.utils - INFO - Reload module pynqimport from .py file\n",
      "distributed.deploy.ssh - INFO - distributed.preloading - INFO - Import preload module: pynqimport.py\n",
      "distributed.deploy.ssh - INFO - distributed.worker - INFO -       Start worker at:   tcp://10.1.212.126:41381\n",
      "Task exception was never retrieved\n",
      "future: <Task finished coro=<_wrap_awaitable() done, defined at /opt/tools/external/anaconda/envs/pynq-dask/lib/python3.7/asyncio/tasks.py:623> exception=PermissionDenied('Permission denied')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/tools/external/anaconda/envs/pynq-dask/lib/python3.7/asyncio/tasks.py\", line 630, in _wrap_awaitable\n",
      "    return (yield from awaitable.__await__())\n",
      "  File \"/home/ldierick/.local/lib/python3.7/site-packages/distributed/deploy/spec.py\", line 59, in _\n",
      "    await self.start()\n",
      "  File \"/mnt/scratch/ldierick/octoray/fwi/SSHCluster.py\", line 118, in start\n",
      "    self.connection = await asyncssh.connect(self.address, **connect_options)\n",
      "  File \"/home/ldierick/.local/lib/python3.7/site-packages/asyncssh/connection.py\", line 7690, in connect\n",
      "    timeout=new_options.connect_timeout)\n",
      "  File \"/opt/tools/external/anaconda/envs/pynq-dask/lib/python3.7/asyncio/tasks.py\", line 414, in wait_for\n",
      "    return await fut\n",
      "  File \"/home/ldierick/.local/lib/python3.7/site-packages/asyncssh/connection.py\", line 439, in _connect\n",
      "    await options.waiter\n",
      "asyncssh.misc.PermissionDenied: Permission denied\n"
     ]
    },
    {
     "ename": "PermissionDenied",
     "evalue": "Permission denied",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionDenied\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6767deb77e39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Finally, add the kernels you want to execute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mdata_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernels_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moctoray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msingle_cu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Execute the added kernels on the workers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/scratch/ldierick/octoray/fwi/Octoray.py\u001b[0m in \u001b[0;36msetup_cluster\u001b[0;34m(self, data, *kernels)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m#create the cluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernels_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/scratch/ldierick/octoray/fwi/Octoray.py\u001b[0m in \u001b[0;36mcreate_cluster\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m                                       \u001b[0mworker_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"nthreads\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"n_workers\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"preload\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"pynqimport.py\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"nanny\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                                       \u001b[0mworker_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"distributed.Worker\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                                       \u001b[0mscheduler_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"host\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"port\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler_port\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                                      )\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/scratch/ldierick/octoray/fwi/SSHCluster.py\u001b[0m in \u001b[0;36mOctoSSHCluster\u001b[0;34m(hosts, connect_options, worker_options, scheduler_options, worker_module, worker_class, remote_python, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m         },\n\u001b[1;32m    349\u001b[0m     }\n\u001b[0;32m--> 350\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mSpecCluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"SSHCluster\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/distributed/deploy/spec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, workers, scheduler, worker, asynchronous, loop, security, silence_logs, name, shutdown_on_close, scheduler_sync_interval)\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop_runner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_correct_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             return sync(\n\u001b[0;32m--> 310\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m             )\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tools/external/anaconda/envs/pynq-dask/lib/python3.7/site-packages/tornado/gen.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                         \u001b[0mexc_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/distributed/deploy/spec.py\u001b[0m in \u001b[0;36m_correct_state_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                     \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cluster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m                     \u001b[0;32mawait\u001b[0m \u001b[0mw\u001b[0m  \u001b[0;31m# for tornado gen.coroutine support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_open\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/distributed/deploy/spec.py\u001b[0m in \u001b[0;36m_\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                     \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m                     \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/scratch/ldierick/octoray/fwi/SSHCluster.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"cd \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dir\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" && \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masyncssh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mconnect_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"uname\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/asyncssh/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(host, port, tunnel, family, flags, local_addr, config, options, **kwargs)\u001b[0m\n\u001b[1;32m   7688\u001b[0m         _connect(new_options, loop, flags, conn_factory,\n\u001b[1;32m   7689\u001b[0m                  'Opening SSH connection to'),\n\u001b[0;32m-> 7690\u001b[0;31m         timeout=new_options.connect_timeout)\n\u001b[0m\u001b[1;32m   7691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tools/external/anaconda/envs/pynq-dask/lib/python3.7/asyncio/tasks.py\u001b[0m in \u001b[0;36mwait_for\u001b[0;34m(fut, timeout, loop)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/asyncssh/connection.py\u001b[0m in \u001b[0;36m_connect\u001b[0;34m(options, loop, flags, conn_factory, msg)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0;32mawait\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaiter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m         \u001b[0mfree_conn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionDenied\u001b[0m: Permission denied"
     ]
    }
   ],
   "source": [
    "# first load in the data\n",
    "import json\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "# Load in data and config settings\n",
    "data = []\n",
    "with open(DIR_PATH+\"input/\"+\"10x10_100\"+\".txt\") as f:\n",
    "    for l in f:\n",
    "        data.append(float(l))\n",
    "\n",
    "config = None\n",
    "with open(DIR_PATH+\"input/GenericInput.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "#set specific configurations for different types of kernels\n",
    "single_cu_config = config\n",
    "double_cu_config = copy.deepcopy(config)\n",
    "single_cu_config[\"ngrid\"][\"x\"]=50\n",
    "double_cu_config[\"ngrid\"][\"x\"]=25\n",
    "\n",
    "# Configure the kernels by specifying the path to the bitstream, number of compute units, batchsize per compute unit and the function names and variables with their respective memory banks.\n",
    "single_cu = octoray.create_kernel(XCLBIN_PATH_DEFAULT,1,500,[[{\"dotprod_1\":[\"HBM0\",\"HBM1\",\"HBM2\"]},{\"update_1\":[\"HBM3\",\"HBM4\",\"HBM5\"]}]],single_cu_config)\n",
    "\n",
    "double_cu = octoray.create_kernel(XCLBIN_PATH_MULTCU,2,250,[[{\"dotprod_1\":[\"HBM0\",\"HBM1\",\"HBM2\"]},{\"update_1\":[\"HBM6\",\"HBM7\",\"HBM8\"]}],\n",
    "                                                [{\"dotprod_2\":[\"HBM3\",\"HBM4\",\"HBM5\"]},{\"update_2\":[\"HBM9\",\"HBM10\",\"HBM11\"]}]],double_cu_config)\n",
    "\n",
    "# Finally, add the kernels you want to execute\n",
    "data_split, kernels_split = octoray.setup_cluster(data,single_cu)\n",
    "\n",
    "# Execute the added kernels on the workers.\n",
    "results = octoray.execute_function(execute_function,data_split,kernels_split)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Graceful shutdown for OpenSSH version >= 7.9 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "octoray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Ungraceful shutdown for OpenSSH version < 7.9 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await octoray.fshutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
