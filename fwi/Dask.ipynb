{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Introduction </h1>\n",
    "\n",
    "This notebook demonstrates how to scale an implemenation of Full Waveform Inversion using multiple FPGAs with bitstreams containing one or multiple copied compute units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Define experiment parameters: </h3>\n",
    "    \n",
    "XCLBIN_PATH_DEFAULT => Default path for the .xclbin file containing 1 compute unit  \n",
    "XCLBIN_PATH_MULTCU => Default path for .xclbin file containing 2 copied compute units  \n",
    "XRT_ENV_PATH => Path to the Xilinx Runtime setup script.  \n",
    "DEVICE_NAME_DEFAULT => Default name for the FPGA device  \n",
    "SPAWN_PATH => Location on the host machines where the worker is spawned.\n",
    "DIR_PATH => Path to the directory containing the FWI input files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "XCLBIN_PATH_DEFAULT = \"bitstreams/u280_xclbin/500_500_HBM/FullW.xclbin\"\n",
    "XCLBIN_PATH_MULTCU = \"bitstreams/u280_xclbin/500_250_HBM/FullW.xclbin\"\n",
    "XRT_ENV_PATH = \"/opt/xilinx/xrt/setup.sh\"\n",
    "DEVICE_NAME_DEFAULT=\"xilinx_u280_xdma_201920_3\"\n",
    "SPAWN_PATH = \"/mnt/scratch/ldierick/octoray/fwi\"\n",
    "DIR_PATH_FWI = \"default/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are more than 1 host, the connect_options and worker_options can be lists of dictionaries containing the different specifications per worker. The list and hosts are matched in the order they are specified and the lists must be the same length.  \n",
    "\n",
    "The pynqimport.py file reads the bitstream from config[\"overlay\"] and downloads it to the host machine.\n",
    "\n",
    "**Write the config to the cluster_config.json file before instantiating the workers, or the Overlay might not match** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "config = {\n",
    "    \"scheduler\":\"10.1.212.129\",\n",
    "    \"hosts\":[\"10.1.212.126\",\"10.1.212.127\"],\n",
    "    \"connect_options\":{\"port\":22,\"xrt\":XRT_ENV_PATH,\"dir\":SPAWN_PATH},\n",
    "    \"worker_options\":{\"nthreads\":1,\"n_workers\":1,\"preload\":\"pynqimport.py\",\"nanny\":\"0\",\"memory_limit\":0},\n",
    "    \"scheduler_options\":{\"port\":8786},\n",
    "    \"worker_class\":\"distributed.Worker\",\n",
    "    \"overlay\": XCLBIN_PATH_DEFAULT\n",
    "}\n",
    "\n",
    "with open(\"cluster_config.json\",\"w\") as f:\n",
    "    json.dump(config,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Define the worker method </h3> \n",
    "\n",
    "Here, we define the Python method which will be executed on each of the Dask workers. This function calls the driver using the data partition it receives, and returns the output data (along with some performance statistics) to the caller (the Dask client). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_function(grid_data,kernel,id):\n",
    "    import numpy as np\n",
    "    import time\n",
    "    from pynq import Overlay, allocate, Device, lib\n",
    "    from FWIDriver import FWI\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Set up the configuration\n",
    "    cu = kernel[\"compute_unit\"]\n",
    "    config = kernel[\"config\"]\n",
    "    path = kernel[\"path_to_kernel\"]\n",
    "    grid_data = grid_data\n",
    "    resolution = config[\"Freq\"][\"nTotal\"] * config[\"nSources\"] * config[\"nReceivers\"]\n",
    "    gridsize = len(grid_data)   \n",
    "    config[\"tolerance\"] = 9.99*10**-7\n",
    "    config[\"max\"] = 1000\n",
    "\n",
    "    # Load the overlay\n",
    "    devices = Device.devices\n",
    "    \n",
    "    # Get the Overlay\n",
    "    #TODO: add device name to config file\n",
    "    ol = Overlay(path, download=False, device=devices[0])\n",
    "\n",
    "    # Allocate the buffers\n",
    "    A = allocate(shape=(resolution,gridsize), dtype=np.complex64, target=getattr(ol,kernel[\"functions\"][0][\"dotprod_\"+str(cu)][0]))\n",
    "    B = allocate(shape=(gridsize,), dtype=np.float32, target=getattr(ol,kernel[\"functions\"][0][\"dotprod_\"+str(cu)][1]))\n",
    "    C = allocate(shape=(resolution,), dtype=np.complex64, target=getattr(ol,kernel[\"functions\"][0][\"dotprod_\"+str(cu)][2]))\n",
    "\n",
    "    D = allocate(shape=(resolution,gridsize), dtype=np.complex64,  target=getattr(ol,kernel[\"functions\"][1][\"update_\"+str(cu)][0]))\n",
    "    E = allocate(shape=(resolution,),dtype=np.complex64,  target=getattr(ol,kernel[\"functions\"][1][\"update_\"+str(cu)][1]))\n",
    "    F = allocate(shape=(gridsize), dtype=np.complex64, target=getattr(ol,kernel[\"functions\"][1][\"update_\"+str(cu)][2]))\n",
    "\n",
    "    # set up the kernel IP's\n",
    "    dotprod = getattr(ol,\"dotprod_\"+str(cu))\n",
    "    update = getattr(ol,\"update_\"+str(cu))\n",
    "    \n",
    "    \n",
    "    # Execute the Full Waveform Inversion algorithm\n",
    "    fwi = FWI(A,B,C,D,E,F,dotprod,update,config,resolution,gridsize,True)\n",
    "\n",
    "    fwi.pre_process(grid_data)\n",
    "\n",
    "    # reconstruct the grid by performing Full Wavefrom Inversion\n",
    "    chi = fwi.reconstruct()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    # free all the buffers\n",
    "    A.freebuffer()\n",
    "    B.freebuffer()\n",
    "    C.freebuffer()\n",
    "    D.freebuffer()\n",
    "    E.freebuffer()\n",
    "    F.freebuffer()\n",
    "        \n",
    "    # Return statistics and results from FWI\n",
    "    \n",
    "    dict_t = {\n",
    "    \"id\": id,\n",
    "    \"cu\": cu,\n",
    "    \"time\": total_time,\n",
    "    \"chi\":chi,\n",
    "    }\n",
    "    return dict_t\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> SSH helper function </h2>\n",
    "\n",
    "https://www.ssh.com/academy/ssh/copy-id\n",
    "\n",
    "Sometimes we need to connect with a password once. We can use this helper function to login to all the hosts once and use passwordless authentication afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio, asyncssh\n",
    "import json\n",
    "    \n",
    "for h in config[\"hosts\"]:\n",
    "    async with asyncssh.connect(h,22,password=\"\",username=\"\") as conn:\n",
    "        res = await conn.run(\"uname\")\n",
    "        print(res.stdout,end='')\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Set up Octoray </h2>\n",
    "\n",
    "Set up the Octoray framework. A user can pass either a dict or a config file containing a dict and specifiy if the scheduler and workers need to be set up or are already instantiated manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Octoray import Octoray\n",
    "\n",
    "# Create an octoray instance with the \n",
    "octoray = Octoray(ssh_cluster=True,config_file=config)\n",
    "\n",
    "\n",
    "# first load in the data\n",
    "import json\n",
    "import copy\n",
    "import time\n",
    "\n",
    "# Load in data and config settings\n",
    "data = []\n",
    "with open(DIR_PATH_FWI+\"input/\"+\"10x10_100\"+\".txt\") as f:\n",
    "    for l in f:\n",
    "        data.append(float(l))\n",
    "        \n",
    "data.extend(data)\n",
    "\n",
    "fwi_config = None\n",
    "with open(DIR_PATH_FWI+\"input/GenericInput.json\") as f:\n",
    "    fwi_config = json.load(f)\n",
    "\n",
    "#set specific configurations for different types of kernels\n",
    "single_cu_config = fwi_config\n",
    "double_cu_config = copy.deepcopy(fwi_config)\n",
    "single_cu_config[\"ngrid\"][\"x\"]=50\n",
    "double_cu_config[\"ngrid\"][\"x\"]=25\n",
    "\n",
    "# Configure the kernels by specifying the path to the bitstream, number of compute units, batchsize per compute unit and the function names and variables with their respective memory banks.\n",
    "single_cu = octoray.create_kernel(XCLBIN_PATH_DEFAULT,1,500,[[{\"dotprod_1\":[\"HBM0\",\"HBM1\",\"HBM2\"]},{\"update_1\":[\"HBM3\",\"HBM4\",\"HBM5\"]}]],single_cu_config)\n",
    "\n",
    "double_cu = octoray.create_kernel(XCLBIN_PATH_MULTCU,2,250,[[{\"dotprod_1\":[\"HBM0\",\"HBM1\",\"HBM2\"]},{\"update_1\":[\"HBM6\",\"HBM7\",\"HBM8\"]}],\n",
    "                                                [{\"dotprod_2\":[\"HBM3\",\"HBM4\",\"HBM5\"]},{\"update_2\":[\"HBM9\",\"HBM10\",\"HBM11\"]}]],double_cu_config)\n",
    "\n",
    "# Finally, add the kernels you want to execute, in this case we add one single compute unit kernel per host.\n",
    "kernels = []\n",
    "for i in range(len(octoray.hosts)):\n",
    "    kernels.append(copy.deepcopy(double_cu))\n",
    "    \n",
    "data_split, kernels_split = octoray.setup_cluster(data,*kernels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, progress\n",
    "\n",
    "client = Client(\"tcp://10.1.212.127:8786\")\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Execute the added kernels on the host machines </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "\n",
    "result = octoray.execute_hybrid(execute_function,data_split,kernels_split)\n",
    "\n",
    "print(result)\n",
    "print(time.time()-t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Graceful shutdown for OpenSSH version >= 7.9 </h3>\n",
    " \n",
    "Doesn't work on XACC yet... (UNTESTED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "octoray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Ungraceful shutdown for OpenSSH version < 7.9 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await octoray.fshutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
